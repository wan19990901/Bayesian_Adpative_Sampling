# DPO Training Configuration

# Model settings
model_name_or_path: "Qwen/Qwen1.5-7B"
ref_model: "Qwen/Qwen1.5-7B"

# Training settings
learning_rate: 2e-5
num_train_epochs: 3
per_device_train_batch_size: 4
per_device_eval_batch_size: 4
gradient_accumulation_steps: 16
max_length: 2048
max_prompt_length: 1024

# Optimization settings
lr_scheduler_type: "cosine"
warmup_steps: 100
weight_decay: 0.01
gradient_checkpointing: true
bf16: true

# Logging and evaluation
logging_steps: 10
save_steps: 100
eval_steps: 100
report_to: "wandb"

# DPO specific settings
loss_type: "sigmoid"
label_smoothing: 0.1
choose_type: "max_min"  # Options: max_min, dynamic, first, best

# Dataset settings
train_file: "data/math-7500/train.jsonl"
eval_file: "data/math-7500/eval.jsonl"

# Output settings
output_dir: "outputs/dpo_training"
run_name: "dpo_training_run"
